{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mouth_height(top_lip, bottom_lip):\n",
    "    sum=0\n",
    "    for i in [8,9,10]:\n",
    "        # distance between two near points up and down\n",
    "        distance = math.sqrt( (top_lip[i][0] - bottom_lip[18-i][0])**2 + \n",
    "                              (top_lip[i][1] - bottom_lip[18-i][1])**2   )\n",
    "        sum += distance\n",
    "    return sum / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_lip height: 3.05\n",
      "bottom_lip height: 5.07\n",
      "mouth height: 0.67\n",
      "Is mouth open: False\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from mouth_open_algorithm import get_lip_height, get_mouth_height, check_mouth_open\n",
    "\n",
    "# obama open mouth\n",
    "#top_lip = [(181, 359), (192, 339), (211, 332), (225, 336), (243, 333), (271, 342), (291, 364), (282, 363), (242, 346), (225, 347), (211, 345), (188, 358)]\n",
    "#bottom_lip = [(291, 364), (270, 389), (243, 401), (223, 403), (207, 399), (190, 383), (181, 359), (188, 358), (210, 377), (225, 381), (243, 380), (282, 363)]\n",
    "\n",
    "# close mouth\n",
    "#top_lip = [(151, 127), (157, 126), (163, 126), (168, 127), (172, 127), (178, 127), (185, 129), (182, 129), (172, 130), (167, 130), (163, 129), (153, 127)]\n",
    "#bottom_lip = [(185, 129), (177, 133), (171, 135), (166, 135), (161, 134), (156, 132), (151, 127), (153, 127), (162, 129), (167, 130), (171, 130), (182, 129)]\n",
    "\n",
    "print('top_lip height: %.2f' % get_lip_height(top_lip))\n",
    "print('bottom_lip height: %.2f' % get_lip_height(bottom_lip))\n",
    "print('mouth height: %.2f' % get_mouth_height(top_lip,bottom_lip))\n",
    "print('Is mouth open:', check_mouth_open(top_lip,bottom_lip) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\PIL\\Image.py:3240\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3239\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3240\u001b[0m     fp\u001b[39m.\u001b[39;49mseek(\u001b[39m0\u001b[39m)\n\u001b[0;32m   3241\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mAttributeError\u001b[39;00m, io\u001b[39m.\u001b[39mUnsupportedOperation):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m success, images \u001b[39m=\u001b[39m webcam\u001b[39m.\u001b[39mread()\n\u001b[0;32m      8\u001b[0m \u001b[39m# Load the jpg file into a numpy array\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m image \u001b[39m=\u001b[39m face_recognition\u001b[39m.\u001b[39;49mload_image_file(images)\n\u001b[0;32m     11\u001b[0m \u001b[39m# Find all facial features in all the faces in the image\u001b[39;00m\n\u001b[0;32m     12\u001b[0m face_landmarks_list \u001b[39m=\u001b[39m face_recognition\u001b[39m.\u001b[39mface_landmarks(image)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\face_recognition\\api.py:86\u001b[0m, in \u001b[0;36mload_image_file\u001b[1;34m(file, mode)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_image_file\u001b[39m(file, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     79\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[39m    Loads an image file (.jpg, .png, etc) into a numpy array\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39m    :return: image contents as numpy array\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m     im \u001b[39m=\u001b[39m PIL\u001b[39m.\u001b[39;49mImage\u001b[39m.\u001b[39;49mopen(file)\n\u001b[0;32m     87\u001b[0m     \u001b[39mif\u001b[39;00m mode:\n\u001b[0;32m     88\u001b[0m         im \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39mconvert(mode)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\PIL\\Image.py:3242\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3240\u001b[0m     fp\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n\u001b[0;32m   3241\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mAttributeError\u001b[39;00m, io\u001b[39m.\u001b[39mUnsupportedOperation):\n\u001b[1;32m-> 3242\u001b[0m     fp \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO(fp\u001b[39m.\u001b[39;49mread())\n\u001b[0;32m   3243\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   3245\u001b[0m prefix \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39mread(\u001b[39m16\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import face_recognition\n",
    "import cv2\n",
    "\n",
    "#webcam = cv2.VideoCapture(1)\n",
    "#success, images = webcam.read()\n",
    "images = \n",
    "\n",
    "# Load the jpg file into a numpy array\n",
    "image = face_recognition.load_image_file(images)\n",
    "\n",
    "# Find all facial features in all the faces in the image\n",
    "face_landmarks_list = face_recognition.face_landmarks(image)\n",
    "\n",
    "print(\"I found {} face(s) in this photograph.\".format(len(face_landmarks_list)))\n",
    "\n",
    "pil_image = Image.fromarray(image)\n",
    "d = ImageDraw.Draw(pil_image)\n",
    "\n",
    "for face_landmarks in face_landmarks_list:\n",
    "    # Print the location of each facial feature in this image\n",
    "    facial_features = [\n",
    "        'chin',\n",
    "        'left_eyebrow',\n",
    "        'right_eyebrow',\n",
    "        'nose_bridge',\n",
    "        'nose_tip',\n",
    "        'left_eye',\n",
    "        'right_eye',\n",
    "        'top_lip',\n",
    "        'bottom_lip'\n",
    "    ]\n",
    "\n",
    "    for facial_feature in facial_features:\n",
    "        print(\"The {} in this face has the following points: {}\".format(facial_feature, face_landmarks[facial_feature]))\n",
    "\n",
    "    # Let's trace out each facial feature in the image with a line!\n",
    "    for facial_feature in facial_features:\n",
    "        d.line(face_landmarks[facial_feature], width=5)\n",
    "\n",
    "# Display drawed image\n",
    "cv2.imshow(\"Face Detection\", image)\n",
    "cv2.waitKey(0)\n",
    "#pil_image.save('test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_lip_height: 10.23, bottom_lip_height: 15.83, mouth_height: 21.50, min*ratio: 5.11\n",
      "top_lip_height: 13.77, bottom_lip_height: 15.90, mouth_height: 4.45, min*ratio: 6.89\n",
      "top_lip_height: 9.04, bottom_lip_height: 13.03, mouth_height: 6.49, min*ratio: 4.52\n",
      "top_lip_height: 11.36, bottom_lip_height: 12.74, mouth_height: 2.85, min*ratio: 5.68\n",
      "top_lip_height: 10.35, bottom_lip_height: 11.03, mouth_height: 3.33, min*ratio: 5.17\n",
      "top_lip_height: 11.00, bottom_lip_height: 12.04, mouth_height: 3.33, min*ratio: 5.50\n",
      "top_lip_height: 10.00, bottom_lip_height: 12.03, mouth_height: 5.03, min*ratio: 5.00\n",
      "top_lip_height: 12.01, bottom_lip_height: 12.71, mouth_height: 3.39, min*ratio: 6.01\n",
      "top_lip_height: 9.35, bottom_lip_height: 11.03, mouth_height: 2.00, min*ratio: 4.67\n",
      "top_lip_height: 11.02, bottom_lip_height: 13.01, mouth_height: 4.33, min*ratio: 5.51\n",
      "top_lip_height: 10.33, bottom_lip_height: 13.03, mouth_height: 6.00, min*ratio: 5.17\n",
      "top_lip_height: 11.33, bottom_lip_height: 14.01, mouth_height: 5.07, min*ratio: 5.67\n",
      "top_lip_height: 9.33, bottom_lip_height: 12.68, mouth_height: 25.68, min*ratio: 4.67\n",
      "top_lip_height: 9.02, bottom_lip_height: 12.36, mouth_height: 29.01, min*ratio: 4.51\n",
      "top_lip_height: 9.00, bottom_lip_height: 13.36, mouth_height: 32.00, min*ratio: 4.50\n",
      "top_lip_height: 9.00, bottom_lip_height: 9.70, mouth_height: 6.00, min*ratio: 4.50\n",
      "top_lip_height: 10.02, bottom_lip_height: 11.02, mouth_height: 1.67, min*ratio: 5.01\n",
      "top_lip_height: 10.02, bottom_lip_height: 13.01, mouth_height: 2.33, min*ratio: 5.01\n",
      "top_lip_height: 9.67, bottom_lip_height: 12.01, mouth_height: 0.00, min*ratio: 4.83\n",
      "top_lip_height: 9.00, bottom_lip_height: 16.02, mouth_height: 23.35, min*ratio: 4.50\n",
      "top_lip_height: 10.00, bottom_lip_height: 17.02, mouth_height: 30.67, min*ratio: 5.00\n",
      "top_lip_height: 10.67, bottom_lip_height: 16.70, mouth_height: 27.34, min*ratio: 5.33\n",
      "top_lip_height: 10.67, bottom_lip_height: 18.35, mouth_height: 31.68, min*ratio: 5.33\n",
      "top_lip_height: 13.01, bottom_lip_height: 17.36, mouth_height: 5.75, min*ratio: 6.51\n",
      "top_lip_height: 12.67, bottom_lip_height: 19.38, mouth_height: 1.33, min*ratio: 6.33\n",
      "top_lip_height: 11.69, bottom_lip_height: 14.36, mouth_height: 3.75, min*ratio: 5.85\n",
      "top_lip_height: 13.04, bottom_lip_height: 15.68, mouth_height: 2.67, min*ratio: 6.52\n",
      "top_lip_height: 11.38, bottom_lip_height: 15.02, mouth_height: 4.67, min*ratio: 5.69\n",
      "top_lip_height: 12.04, bottom_lip_height: 15.72, mouth_height: 5.00, min*ratio: 6.02\n",
      "top_lip_height: 13.06, bottom_lip_height: 16.05, mouth_height: 3.37, min*ratio: 6.53\n",
      "top_lip_height: 9.39, bottom_lip_height: 13.04, mouth_height: 2.67, min*ratio: 4.69\n",
      "top_lip_height: 9.39, bottom_lip_height: 13.04, mouth_height: 4.04, min*ratio: 4.69\n",
      "top_lip_height: 9.68, bottom_lip_height: 13.04, mouth_height: 4.00, min*ratio: 4.84\n",
      "top_lip_height: 12.71, bottom_lip_height: 17.38, mouth_height: 5.67, min*ratio: 6.35\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m face_locations \u001b[39m=\u001b[39m face_recognition\u001b[39m.\u001b[39mface_locations(frame)\n\u001b[0;32m     42\u001b[0m face_encodings \u001b[39m=\u001b[39m face_recognition\u001b[39m.\u001b[39mface_encodings(frame, face_locations)\n\u001b[1;32m---> 43\u001b[0m face_landmarks_list \u001b[39m=\u001b[39m face_recognition\u001b[39m.\u001b[39;49mface_landmarks(frame)\n\u001b[0;32m     45\u001b[0m \u001b[39m# Loop through each face in this frame of video\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[39mfor\u001b[39;00m (top, right, bottom, left), face_encoding, face_landmarks \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(face_locations, face_encodings, face_landmarks_list):\n\u001b[0;32m     47\u001b[0m \n\u001b[0;32m     48\u001b[0m     \u001b[39m#  See if the face is a match for the known face(s)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\face_recognition\\api.py:177\u001b[0m, in \u001b[0;36mface_landmarks\u001b[1;34m(face_image, face_locations, model)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mface_landmarks\u001b[39m(face_image, face_locations\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlarge\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    169\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[39m    Given an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[39m    :return: A list of dicts of face feature locations (eyes, nose, etc)\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 177\u001b[0m     landmarks \u001b[39m=\u001b[39m _raw_face_landmarks(face_image, face_locations, model)\n\u001b[0;32m    178\u001b[0m     landmarks_as_tuples \u001b[39m=\u001b[39m [[(p\u001b[39m.\u001b[39mx, p\u001b[39m.\u001b[39my) \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m landmark\u001b[39m.\u001b[39mparts()] \u001b[39mfor\u001b[39;00m landmark \u001b[39min\u001b[39;00m landmarks]\n\u001b[0;32m    180\u001b[0m     \u001b[39m# For a definition of each point index, see https://cdn-images-1.medium.com/max/1600/1*AbEg31EgkbXSQehuNJBlWg.png\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\face_recognition\\api.py:156\u001b[0m, in \u001b[0;36m_raw_face_landmarks\u001b[1;34m(face_image, face_locations, model)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_raw_face_landmarks\u001b[39m(face_image, face_locations\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlarge\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    155\u001b[0m     \u001b[39mif\u001b[39;00m face_locations \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 156\u001b[0m         face_locations \u001b[39m=\u001b[39m _raw_face_locations(face_image)\n\u001b[0;32m    157\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m         face_locations \u001b[39m=\u001b[39m [_css_to_rect(face_location) \u001b[39mfor\u001b[39;00m face_location \u001b[39min\u001b[39;00m face_locations]\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\face_recognition\\api.py:105\u001b[0m, in \u001b[0;36m_raw_face_locations\u001b[1;34m(img, number_of_times_to_upsample, model)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[39mreturn\u001b[39;00m cnn_face_detector(img, number_of_times_to_upsample)\n\u001b[0;32m    104\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     \u001b[39mreturn\u001b[39;00m face_detector(img, number_of_times_to_upsample)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "from mouth_open_algorithm import get_lip_height, get_mouth_height\n",
    "from datetime import datetime\n",
    "\n",
    "def is_mouth_open(face_landmarks):\n",
    "    top_lip = face_landmarks['top_lip']\n",
    "    bottom_lip = face_landmarks['bottom_lip']\n",
    "\n",
    "    top_lip_height = get_lip_height(top_lip)\n",
    "    bottom_lip_height = get_lip_height(bottom_lip)\n",
    "    mouth_height = get_mouth_height(top_lip, bottom_lip)\n",
    "    \n",
    "    # if mouth is open more than lip height * ratio, return true.\n",
    "    ratio = 0.5\n",
    "    print('top_lip_height: %.2f, bottom_lip_height: %.2f, mouth_height: %.2f, min*ratio: %.2f' \n",
    "          % (top_lip_height,bottom_lip_height,mouth_height, min(top_lip_height, bottom_lip_height) * ratio))\n",
    "          \n",
    "    if mouth_height > min(top_lip_height, bottom_lip_height) * ratio:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Get a reference to webcam #0 (the default one)\n",
    "video_capture = cv2.VideoCapture(1)\n",
    "\n",
    "# Define the codec and create VideoWriter object to save video to local\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID') # codec\n",
    "# cv2.VideoWriter( filename, fourcc, fps, frameSize )\n",
    "out = cv2.VideoWriter('output.avi', fourcc, 7, (640, 480)) \n",
    "\n",
    "# Load a sample picture and learn how to recognize it.\n",
    "pk_image = face_recognition.load_image_file(\"PK2.jpg\") # replace peter.jpg with your own image !!\n",
    "pk_face_encoding = face_recognition.face_encodings(pk_image)[0]\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "    # Find all the faces and face enqcodings in the frame of video\n",
    "    face_locations = face_recognition.face_locations(frame)\n",
    "    face_encodings = face_recognition.face_encodings(frame, face_locations)\n",
    "    face_landmarks_list = face_recognition.face_landmarks(frame)\n",
    "\n",
    "    # Loop through each face in this frame of video\n",
    "    for (top, right, bottom, left), face_encoding, face_landmarks in zip(face_locations, face_encodings, face_landmarks_list):\n",
    "\n",
    "        #  See if the face is a match for the known face(s)\n",
    "        match = face_recognition.compare_faces([pk_face_encoding], face_encoding)\n",
    "\n",
    "        name = \"Unknown\"\n",
    "        if match[0]:\n",
    "            name = \"PK\"\n",
    "\n",
    "        # Draw a box around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        cv2.rectangle(frame, (left, bottom), (right, bottom + 35), (0, 0, 255), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (left + 6, bottom + 25), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "\n",
    "        # Display text for mouth open / close\n",
    "        ret_mouth_open = is_mouth_open(face_landmarks)\n",
    "        if ret_mouth_open is True:\n",
    "            text = 'Mouth is open'\n",
    "        else:\n",
    "            text = 'Open your mouth'\n",
    "        cv2.putText(frame, text, (left, top - 50), cv2.FONT_HERSHEY_DUPLEX, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "    out.write(frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I found 1 face(s) in this photograph.\n",
      "The chin in this face has the following points: [(1524, 1994), (1523, 2124), (1538, 2263), (1574, 2405), (1631, 2539), (1714, 2659), (1807, 2760), (1909, 2829), (2047, 2837), (2205, 2810), (2358, 2757), (2500, 2674), (2620, 2558), (2694, 2406), (2720, 2233), (2716, 2054), (2707, 1878)]\n",
      "The left_eyebrow in this face has the following points: [(1534, 1864), (1567, 1783), (1644, 1741), (1741, 1723), (1837, 1735)]\n",
      "The right_eyebrow in this face has the following points: [(1987, 1699), (2103, 1643), (2231, 1624), (2355, 1652), (2459, 1719)]\n",
      "The nose_bridge in this face has the following points: [(1921, 1872), (1920, 1957), (1915, 2040), (1910, 2129)]\n",
      "The nose_tip in this face has the following points: [(1831, 2267), (1888, 2276), (1949, 2277), (2017, 2262), (2081, 2243)]\n",
      "The left_eye in this face has the following points: [(1632, 1962), (1682, 1917), (1749, 1905), (1820, 1941), (1759, 1965), (1690, 1976)]\n",
      "The right_eye in this face has the following points: [(2108, 1890), (2176, 1831), (2255, 1821), (2336, 1841), (2268, 1878), (2188, 1892)]\n",
      "The top_lip in this face has the following points: [(1772, 2502), (1832, 2433), (1898, 2392), (1956, 2401), (2017, 2375), (2128, 2395), (2256, 2438), (2214, 2443), (2029, 2449), (1963, 2465), (1906, 2467), (1806, 2492)]\n",
      "The bottom_lip in this face has the following points: [(2256, 2438), (2147, 2502), (2049, 2535), (1980, 2549), (1917, 2551), (1847, 2539), (1772, 2502), (1806, 2492), (1908, 2467), (1967, 2463), (2032, 2448), (2214, 2443)]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import face_recognition\n",
    "\n",
    "image_file = 'PK2.jpg' \n",
    "\n",
    "# Load the jpg file into a numpy array\n",
    "image = face_recognition.load_image_file(image_file)\n",
    "\n",
    "# Find all facial features in all the faces in the image\n",
    "face_landmarks_list = face_recognition.face_landmarks(image)\n",
    "\n",
    "print(\"I found {} face(s) in this photograph.\".format(len(face_landmarks_list)))\n",
    "\n",
    "pil_image = Image.fromarray(image)\n",
    "d = ImageDraw.Draw(pil_image)\n",
    "\n",
    "for face_landmarks in face_landmarks_list:\n",
    "    # Print the location of each facial feature in this image\n",
    "    facial_features = [\n",
    "        'chin',\n",
    "        'left_eyebrow',\n",
    "        'right_eyebrow',\n",
    "        'nose_bridge',\n",
    "        'nose_tip',\n",
    "        'left_eye',\n",
    "        'right_eye',\n",
    "        'top_lip',\n",
    "        'bottom_lip'\n",
    "    ]\n",
    "\n",
    "    for facial_feature in facial_features:\n",
    "        print(\"The {} in this face has the following points: {}\".format(facial_feature, face_landmarks[facial_feature]))\n",
    "\n",
    "    # Let's trace out each facial feature in the image with a line!\n",
    "    for facial_feature in facial_features:\n",
    "        d.line(face_landmarks[facial_feature], width=5)\n",
    "\n",
    "# Display drawed image\n",
    "pil_image.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
